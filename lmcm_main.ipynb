{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>LMCM - Project</h1>\n",
    "<h2>Examining System 2 Attention for Factual Reasoning with Irrelevant Context</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from importlib import reload\n",
    "import s2a\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch acc_tests/hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "dataset = \"noise\"\n",
    "model_name = \"GPT2Large\"\n",
    "# for data containing sentences with multiple entities (i.e., more than one person/agent)\n",
    "entity = \"Multi\"\n",
    "multiSem = s2a.load_sttn(model_name, entity, return_dataset=True)\n",
    "multiNeutral = s2a.load_sttn(model_name, entity, sem=False, return_dataset=True)\n",
    "\n",
    "# for data containing sentences with single entity\n",
    "entity = \"Single\"\n",
    "singleSem =  s2a.load_sttn(model_name, entity, return_dataset=True)\n",
    "singleNeutral = s2a.load_sttn(model_name, entity, sem=False, return_dataset=True)\n",
    "\n",
    "#dataset = \"comps\"\n",
    "comps_dist_before = s2a.load_comps(before=True)\n",
    "comps_dist_inbetween = s2a.load_comps(in_between=True)\n",
    "comps_base = s2a.load_comps(base=True)\n",
    "\n",
    "# llama 2\n",
    "LLAMA2_70B_CHAT_HF = 'meta-llama/Llama-2-70b-chat-hf' \n",
    "LLAMA2_13B_CHAT_HF = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "#mistral\n",
    "MISTRAL_INSTRUCT_V02_HF = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# following W&S (2023), baseline, oracle and s2a step 2 share the same system prompt to generate the final answer; the system prompt is loosely based on W&S (2023) Fig. 16\n",
    "system_prompt = 'Complete the following sentence with one single word: '\n",
    "system_prompt = \"State whether the following statement is true or false. Your response should not be dependent on potential nonce words. Separate into two categories labeled with 'Reasoning:' and 'Final answer (true or false):' \\nStatement: \"\n",
    "#system_prompt = \"Is the conclusion of the following statement true or false? Just answer with 'True' or 'False'. Your answer should not depend on potential nonce words. \\nStatement: \" \n",
    "#system_prompt = \"Given the premise and hypothesis provided, does the hypothesis logically follow from the premise? Simply respond with 'True' if the hypothesis can be inferred from the premise, or 'False' if it cannot.\"\n",
    "#system_prompt = \"Given the premise and hypothesis provided, simply respond with 'True' if the hypothesis can be logically inferred from the premise, or 'False' if it cannot.\"\n",
    "system_prompt = \"You are provided with a hypothesis and two premises labeled with 'A' and 'B'. Select the premise which the hypothesis logically follows from. Select 'C', if you're uncertain. Reply with the single letter corresponding to your choice.\"\n",
    "system_prompt = \"Select the premise 1 or 2 from which the provided hypothesis logically follows. Reply with the single number corresponding to your choice.\"\n",
    "system_prompt = \"Select the premise (labeled 'A' and 'B') from which the given hypothesis logically follows. Reply with the single letter corresponding to your choice.\"\n",
    "system_prompt = \"You are a helpful writing assistant. Tell me which sentence 'A' or 'B' is semantically more plausible. Reply with the single letter corresponding to your choice.\"\n",
    "system_prompt = \"You are a helpful writing assistant. Your task is to determine which of two given statements 'A' and 'B' after <<<>>> is semantically more plausible. Potential made-up words should not influence your answer.\\nYou will only respond with 'A' or 'B'. Do not provide any explanations.\"# Reply with the single letter corresponding to your choice.\"\n",
    "\n",
    "#system_prompt = \"You are provided with two premises labeled 'A' and 'B' and a hypothesis. Select the premise which entails the given hypothesis. Please, ignore any potential made-up words.\"# It's only about the logical structure.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 11.00s/it]\n"
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "max_memory_mapping = {0: \"35GB\", 1: \"0GB\", 2: \"0GB\", 3:\"0GB\", 4:\"0GB\", 5:\"0GB\", 6:\"0GB\", 7:\"0GB\"}\n",
    "model, tokenizer = s2a.load_model_hf(MISTRAL_INSTRUCT_V02_HF, memory_pinning=max_memory_mapping, no_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] You are a helpful writing assistant. Tell me, which statement 'A' or 'B' constitutes an entailment. Potential made-up words should not influence your answer.\n",
      "A: A wug is a clam. Therefore, a wug attaches to rocks.\n",
      "B: A wug is a mussel. Therefore, a wug attaches to rocks.\n",
      "\n",
      "Just generate a single letter 'A' or 'B' as your answer: [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/435 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B']\n",
      "['Based on the given information, neither statement A nor statement B entails that a wug attaches to rocks. The statements only mention that a wug is a certain type of animal (clam or mussel), but they do not provide any information about the behavior or characteristics of the wug that would entail that it attaches to rocks. Therefore, neither statement entails that a wug attaches to rocks. Answer: None (0) or N (for \"not entailed', \"In order to determine which statement entails the other, we need to consider whether the conclusion (a wug attaches to rocks) follows logically from the premise (a wug is a scallop or a mussel). Neither statement provides enough information for a wug's attachment behavior to be determined based on its identity alone. Therefore, neither statement A nor statement B entails the other.\\n\\nHowever, if we had additional information that scallops or muss\", 'A. The statement \"A blicket is a mussel\" does not imply that a blicket attaches to rocks, but the statement \"A blicket is a mussel\" and the fact that mussels attach to rocks do go together. However, the statement \"A blicket is a crayfish\" is incompatible with the statement \"A blicket attaches to rocks,\" as crayfish do not attach to rocks in the same way', 'A. The first statement \"A dax is a pigeon\" does not entail that a dax attaches to rocks. However, the second statement \"A dax is a mussel\" does not entail anything about a dax attaching to rocks, but mussels do attach to rocks in reality. Therefore, the statements themselves do not entail the given conclusion about daxes attaching to rocks, but in reality, mussels do attach to rocks', 'Based on the given information, statement B is not an entailment of statement A, and statement A is not an entailment of statement B. However, if we assume that both \"fep\" in statement A and \"fep\" in statement B refer to the same animal species, and that rattlesnakes and iguanas are both known to bask in the sun, then both statements could be true, and statement B would entail statement A. But without this', 'A. The statement \"A dax is a tortoise\" does not necessarily entail that a dax basks in the sun, but the statement \"A dax is an iguana\" does, as iguanas are known to be sun-basking reptiles.', 'Based on the given information, neither statement A nor statement B entails that a fep basks in the sun. Both tarantulas and iguanas can live in various environments and do not necessarily bask in the sun. Therefore, neither statement can be logically deduced from the given premises alone.', 'Based on the given information, statement B is an entailment of statement A if we assume that \"iguana\" is a type of \"fep.\" However, without this assumption, neither statement entails the other. Therefore, neither A nor B is an entailment on its own.\\n\\nHowever, if we consider the additional context that \"fep\" is indeed a type of iguana, then statement B entails statement A. In that case, the answer would be', 'A. An alligator basks in the sun, so a blicket (being an alligator) also basks in the sun.', 'A. The statement \"A dax is a tortoise\" does not imply that a dax basks in the sun, but the statement \"A dax is a tortoise\" does entail that a dax is a type of tortoise, and tortoises are known to bask in the sun. Therefore, the entailment is from statement A to the statement that a dax basks in the sun.', 'A. In A, it is stated that a blicket is a turtle, and turtles are known to bask in the sun. Therefore, if a blicket is a turtle, it follows logically that a blicket basks in the sun.', 'A. In statement A, a blicket is defined as a coffee maker. Coffee makers do not bask in the sun, but the statement \"A blicket basks in the sun\" is added without changing the meaning of the original statement, so it is still true that a blicket is a coffee maker. In statement B, a blicket is defined as an alligator, which is an entirely different type of entity that does bask in the sun,', 'A. In statement A, a blicket is identified as a camel. Camels are known to live in desert environments and bask in the sun. Therefore, the entailment holds true that a blicket basks in the sun. In statement B, however, a blicket is identified as a hippo. Hippos do not bask in the sun; instead, they prefer to stay in water to protect themselves from the sun. Therefore, statement B', 'A. In order for the statement \"a blicket basks in the sun\" to be entailed by the given statement, the blicket must be an animal that basks in the sun. Since a rhino is an animal that can bask in the sun, the statement \"A blicket is a rhino\" entails \"a blicket basks in the sun.\" However, a hippo does not entail that a blicket b', 'A. In statement A, a fep is identified as a giraffe. Giraffes do bask in the sun. Therefore, the statement \"a fep basks in the sun\" is entailed by statement A.\\n\\nIn statement B, a fep is identified as a hippo. Hippos do not bask in the sun. Therefore, the statement \"a fep basks in the sun\" is not entailed by statement B.', 'A. The statement \"A wug is a saxophone\" does not provide enough information to entail that a wug basks in the sun. The statement about a wug basking in the sun is an unrelated fact. However, if we were given a premise that \"A wug is a creature that basks in the sun,\" then the statement \"A wug is a saxophone\" would not entail that a wug b', 'A. A crocodile basks in the sun. Therefore, a fep (which is a crocodile) also basks in the sun.', 'A. The statement \"A wug basks in the sun\" is an entailment of \"A wug is a tortoise,\" because tortoises are known to bask in the sun. However, crocodiles do not typically bask in the sun in the same way that tortoises do.', 'A. A crocodile basks in the sun. Therefore, a fep (which is a crocodile) also basks in the sun.', 'A. The statement \"A wug is a crocodile\" implies that a wug is an animal that can bask in the sun, as crocodiles can do. The statement \"A wug is a cannon\" does not imply that a wug can bask in the sun, as cannons are inanimate objects and cannot bask in the sun.', 'Based on the given information, neither statement A nor statement B entails that a wug basks in the sun. The statements only provide information about the type of animal a wug is, and neither walruses nor seals are known to bask in the sun as a defining characteristic. Therefore, neither statement A nor statement B is an entailment.\\n\\nHowever, if we were to make an assumption that wugs are animals that share characteristics with their real-world counter', 'A. The statement \"A dax is an otter\" does not entail that a dax basks in the sun, but the statement \"A dax is an otter. Otters bask in the sun\" does entail that a dax basks in the sun. However, since the second part of statement A is not given, we cannot determine that from the information provided. Therefore, the answer is A only if we assume that \"otters bask', 'A. A wug basking in the sun is an action typically associated with seals, not plates. Therefore, statement A entails that a wug basks in the sun.', 'A. The statement \"A blicket is a seal\" entails that a blicket is an animal that can bask in the sun, as seals are known to do. However, the statement \"A blicket is a mouse\" does not entail that a blicket basks in the sun, as mice do not have this behavior.', 'A. The statement \"A fep is a lizard\" does not specify whether a fep is a particular type of lizard that basks in the sun or not. However, lizards in general are known to bask in the sun. Therefore, the entailment holds true for statement A.', 'Based on the given information, neither statement A nor statement B entails that a wug basks in the sun. The statements only provide information about the type of animal a wug is, but they do not entail any specific behavior about a wug, such as basking in the sun. Therefore, neither A nor B is the correct answer.', 'Based on the given information, neither statement A nor statement B entails that a wug basks in the sun. The statement \"A wug is a frog\" or \"A wug is a lizard\" does not provide enough information to logically conclude that a wug basks in the sun. Therefore, neither A nor B is the correct answer.', 'A. The statement \"A dax is a lizard\" implies that a dax is a living creature that can bask in the sun, while the statement \"A dax is a revolver\" does not provide any information about the ability of a dax to bask in the sun.', 'A. The statement \"A dax is a monkey\" does not entail that a dax beats its chest. The statement \"A dax is a gorilla,\" on the other hand, does entail that a dax beats its chest, since gorillas are known to beat their chests. However, the given premise in option A does not make that conclusion follow logically.', 'A. The statement \"A blicket is a rhino\" does not imply anything about the behavior of a blicket regarding chest-beating. However, the statement \"A blicket is a rhino\" entails that a blicket is a type of animal, and rhinos are known to exhibit chest-beating behavior in some species. Therefore, if a blicket is a rhino, it could potentially beat its chest based on our knowledge of rhinos', 'A. The statement \"A blicket is a gorilla\" implies that a blicket is an animal that beats its chest. Gorillas are known for this behavior. Therefore, if a blicket is a gorilla, it follows logically that a blicket beats its chest. The statement \"A blicket is an elephant,\" however, does not follow logically from the premise \"A blicket is a gorilla,\" and', 'A. The first statement \"A fep is a butterfly\" does not provide any information about the behavior of a fep, so it cannot entail the statement \"a fep beats its chest.\" However, if we were to assume that \"fep\" is a type of butterfly known to beat its chest, then the entailment could hold. But based on the given information, the answer is A.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "random.seed(42)\n",
    "comps_data = comps_base\n",
    "#sents, targets = (comps_base['sentence_acc'], comps_base['sentence_unacc']), (comps_base['acceptable_concept'], comps_base['unacceptable_concept'])\n",
    "sents = [[acc, unacc] for acc, unacc in zip(comps_data['sentence_acc'], comps_data['sentence_unacc'])] #, (comps_data['acceptable_concept'], comps_data['unacceptable_concept'])\n",
    "for pair in sents:\n",
    "    random.shuffle(pair)\n",
    "targets = [\"A\" if rand[0] == acc else \"B\" for rand, acc in zip(sents, comps_data['sentence_acc'])]\n",
    "\n",
    "data = s2a.preprocess(sents, tokenizer, model.name_or_path, system_prompt, comps=True)\n",
    "#print(tokenizer.convert_ids_to_tokens(data['input_ids'][0], skip_special_tokens=True))\n",
    "\n",
    "N = 1\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(data, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "preds = s2a.generate_hf(model, tokenizer, dataloader, N=N, save_preds=True)\n",
    "print(targets[:N*batch_size])\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁[',\n",
       " 'INST',\n",
       " ']',\n",
       " '▁<<',\n",
       " 'SY',\n",
       " 'S',\n",
       " '>>',\n",
       " '<0x0A>',\n",
       " 'You',\n",
       " \"'\",\n",
       " 'ra',\n",
       " '▁a',\n",
       " '▁helpful',\n",
       " '▁writing',\n",
       " '▁assistant',\n",
       " '.',\n",
       " '▁Tell',\n",
       " '▁me',\n",
       " '▁from',\n",
       " '▁which',\n",
       " '▁prem',\n",
       " 'ise',\n",
       " '▁(',\n",
       " '1',\n",
       " '▁or',\n",
       " '▁',\n",
       " '2',\n",
       " ')',\n",
       " '▁the',\n",
       " '▁given',\n",
       " '▁hypothesis',\n",
       " '▁log',\n",
       " 'ically',\n",
       " '▁follows',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<',\n",
       " '</',\n",
       " 'SY',\n",
       " 'S',\n",
       " '>>',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'PRE',\n",
       " 'MI',\n",
       " 'SE',\n",
       " '▁',\n",
       " '1',\n",
       " ':',\n",
       " '▁A',\n",
       " '▁w',\n",
       " 'ug',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁muss',\n",
       " 'el',\n",
       " '.',\n",
       " '▁',\n",
       " '<0x0A>',\n",
       " 'PRE',\n",
       " 'MI',\n",
       " 'SE',\n",
       " '▁',\n",
       " '2',\n",
       " ':',\n",
       " '▁A',\n",
       " '▁w',\n",
       " 'ug',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁cl',\n",
       " 'am',\n",
       " '.',\n",
       " '▁',\n",
       " '<0x0A>',\n",
       " 'H',\n",
       " 'Y',\n",
       " 'P',\n",
       " 'OT',\n",
       " 'H',\n",
       " 'ES',\n",
       " 'IS',\n",
       " ':',\n",
       " '▁A',\n",
       " '▁w',\n",
       " 'ug',\n",
       " '▁attach',\n",
       " 'es',\n",
       " '▁to',\n",
       " '▁rocks',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'Replace',\n",
       " '▁the',\n",
       " '▁X',\n",
       " '▁by',\n",
       " '▁the',\n",
       " '▁number',\n",
       " '▁corresponding',\n",
       " '▁to',\n",
       " '▁your',\n",
       " '▁choice',\n",
       " '.',\n",
       " '▁',\n",
       " '<0x0A>',\n",
       " 'Answer',\n",
       " ':',\n",
       " '▁The',\n",
       " '▁hypothesis',\n",
       " '▁follows',\n",
       " '▁from',\n",
       " '▁prem',\n",
       " 'ise',\n",
       " '▁X',\n",
       " '.',\n",
       " '▁[',\n",
       " '/',\n",
       " 'INST',\n",
       " ']']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = next(iter(dataloader))['input_ids'][0]\n",
    "tokenizer.convert_ids_to_tokens(instance, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_multDistr = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/multiple_entity_distractor/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n",
    "gpt_multDistrSwapped = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/multiple_entity_distractorSwapped/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n",
    "gpt_multObj = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/multiple_entity_object/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n",
    "gpt_multProf = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/multiple_entity_profession/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n",
    "gpt_multNeutral = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/multiple_entity_with_neutral_distractor/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n",
    "\n",
    "gpt_singleDistr = pd.read_csv('/local/js/lmcm_project/Sorting-Through-The-Noise/data/combined_data/single_entity_distractor/GPT2Large/complete_data_For_MultipleEntityObjectDistractorAccuracyGPT2Large.csv', sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sentences of Object contained in total file: True\n",
      "All sentences of Prof contained in total file: True\n",
      "All identical sentences in Obj and Prof are contained twice in total file: True\n"
     ]
    }
   ],
   "source": [
    "# Check whether all entries of Obj and Prof contained in the total file\n",
    "print(f\"All sentences of Object contained in total file: {gpt_multObj['sentence'].isin(gpt_multDistr['sentence']).all()}\")\n",
    "print(f\"All sentences of Prof contained in total file: {gpt_multProf['sentence'].isin(gpt_multDistr['sentence']).all()}\")\n",
    "\n",
    "# Find identical entries in columns A and B\n",
    "identical_entries = gpt_multObj[gpt_multObj['sentence'].isin(gpt_multProf['sentence'])]['sentence']\n",
    "\n",
    "# Check if all entries occur twice in column C\n",
    "all_entries_twice = all(gpt_multDistr['sentence'].value_counts()[entry] == 2 for entry in identical_entries.tolist())\n",
    "print(f\"All identical sentences in Obj and Prof are contained twice in total file: {all_entries_twice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "s = \"Label: This is a test.\"\n",
    "print(s.split('Label:')[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Through The Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "dataset = \"noise\"\n",
    "# I selected some GPT2* model since my work exclusively examines autoregressive models (other models used in this dataset use masked LM)\n",
    "model_name = \"GPT2Large\"\n",
    "\n",
    "# for data containing sentences with multiple entities (i.e., more than one person/agent)\n",
    "entity = \"Multi\"\n",
    "multiSem = s2a.load_data(model_name, entity, return_dataset=True)\n",
    "multiNeutral = s2a.load_data(model_name, entity, sem=False, return_dataset=True)\n",
    "\n",
    "# for data containing sentences with single entity\n",
    "entity = \"Single\"\n",
    "singleSem =  s2a.load_data(model_name, entity, return_dataset=True)\n",
    "singleNeutral = s2a.load_data(model_name, entity, sem=False, return_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "dataset = \"comps\"\n",
    "comps_dist_before = s2a.load_comps(before=True)\n",
    "ccomps_dist_inbetween = s2a.load_comps(in_between=True)\n",
    "comps_base = s2a.load_comps(base=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12371\n",
      "sentence_inacc\n",
      "A fep is a frog. Therefore, a fep has wings.                  8\n",
      "A blicket is a frog. Therefore, a blicket has talons.         7\n",
      "A wug is a turtle. Therefore, a wug can be found on land.     6\n",
      "A wug is a centipede. Therefore, a wug has six legs.          6\n",
      "A blicket is a worm. Therefore, a blicket is an arthropod.    6\n",
      "                                                             ..\n",
      "A dax is a hummingbird. Therefore, a dax eats meat.           1\n",
      "A blicket is a turtle. Therefore, a blicket eats meat.        1\n",
      "A wug is an avocado. Therefore, a wug eats meat.              1\n",
      "A dax is a trouser. Therefore, a dax eats meat.               1\n",
      "A wug is a clock. Therefore, a wug wears a bridle.            1\n",
      "Name: count, Length: 12371, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(comps_base['sentence_inacc'].unique().size)\n",
    "print(comps_base['sentence_inacc'].value_counts())\n",
    "dupl = comps_base.duplicated(subset=['sentence_inacc'])\n",
    "#print(comps_base[dupl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13896\n",
      "13896\n",
      "sentence_acc            True\n",
      "sentence_inacc          True\n",
      "acceptable_concept      True\n",
      "unacceptable_concept    True\n",
      "distraction_type        True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "unique = ~comps_base.duplicated(subset=['sentence_acc', 'acceptable_concept', 'unacceptable_concept'])\n",
    "unique_df = comps_base[unique]\n",
    "print((unique_df == comps_base).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, targets = (comps_base['sentence_acc'], comps_base['sentence_unacc']), (comps_base['acceptable_concept'], comps_base['unacceptable_concept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 223k/223k [00:00<00:00, 900kB/s]\n",
      "Generating validation split: 100%|██████████| 817/817 [00:00<00:00, 56038.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tqa = s2a.load_qa()\n",
    "dataset = 'qa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = tqa.filter(lambda example: example['category'] == 'History')\n",
    "category['validation'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(set(tqa['validation']['category']))\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared concept</th>\n",
       "      <th>question</th>\n",
       "      <th>irrelevant_context</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>false_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientist</td>\n",
       "      <td>Which scientist developed the theory of genera...</td>\n",
       "      <td>Isaac Newton was an English scientist. Isaac N...</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>Isaac Newton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city</td>\n",
       "      <td>In which city can you find the famous Louvre M...</td>\n",
       "      <td>London is a city in England. London has many f...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explorer</td>\n",
       "      <td>Which explorer is credited with discovering th...</td>\n",
       "      <td>Marco Polo was an Italian explorer. Marco Polo...</td>\n",
       "      <td>Christopher Columbus</td>\n",
       "      <td>Marco Polo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>composer</td>\n",
       "      <td>Which composer wrote the famous symphony known...</td>\n",
       "      <td>Wolfgang Amadeus Mozart was a prolific compose...</td>\n",
       "      <td>Ludwig van Beethoven</td>\n",
       "      <td>Wolfgang Amadeus Mozart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>country</td>\n",
       "      <td>Which country is known for its iconic Sydney O...</td>\n",
       "      <td>Canada is a country in North America. Canada i...</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  shared concept                                           question  \\\n",
       "0      scientist  Which scientist developed the theory of genera...   \n",
       "1           city  In which city can you find the famous Louvre M...   \n",
       "2       explorer  Which explorer is credited with discovering th...   \n",
       "3       composer  Which composer wrote the famous symphony known...   \n",
       "4        country  Which country is known for its iconic Sydney O...   \n",
       "\n",
       "                                  irrelevant_context           true_answer  \\\n",
       "0  Isaac Newton was an English scientist. Isaac N...       Albert Einstein   \n",
       "1  London is a city in England. London has many f...                 Paris   \n",
       "2  Marco Polo was an Italian explorer. Marco Polo...  Christopher Columbus   \n",
       "3  Wolfgang Amadeus Mozart was a prolific compose...  Ludwig van Beethoven   \n",
       "4  Canada is a country in North America. Canada i...             Australia   \n",
       "\n",
       "              false_answer  \n",
       "0             Isaac Newton  \n",
       "1                   London  \n",
       "2               Marco Polo  \n",
       "3  Wolfgang Amadeus Mozart  \n",
       "4                   Canada  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa with ic\n",
    "s2a = reload(s2a)\n",
    "qa_ic = pd.read_csv('/local/js/lmcm_project/factual_qa/qa_ic.csv')\n",
    "qa_ic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama 2\n",
    "LLAMA2_70B_CHAT_HF = 'meta-llama/Llama-2-70b-chat-hf' \n",
    "LLAMA2_13B_CHAT_HF = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "# mistral\n",
    "MIXTRAL_8X7B_INSTRUCT_V01_HF = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "MISTRAL_INSTRUCT_V02_HF = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# gemma\n",
    "GEMMA_7B_IT_HF = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Prompt Templates\n",
    "\n",
    "Three different settings:\n",
    "- Oracle: Zero-shot prompting using zero-attractor sentences only (i.e., no irrelevant context contained) (~ upper-bound)\n",
    "- Baseline: Zero-shot prompting  using a simple system prompt to describe the task and sentences containing irrelevant context (~ lower-bound) \n",
    "- S2A: two-step zero-shot prompting technique that first prompts the model to extract irrelevant context and then conditions on the response to generate final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following W&S (2023), baseline, oracle and s2a step 2 share the same system prompt to generate the final answer; the system prompt is loosely based on W&S (2023) Fig. 16\n",
    "system_prompt = 'Complete the following sentence with one single word: '\n",
    "#system_prompt_comps = \"State whether the following sentence is true or false. Separate into two categories labeled with 'Reasoning:' and 'Final answer (true or false):' \\nSentence: \"\n",
    "system_prompt_comps = \"Is the conclusion of the following statement true or false? Just answer with 'True' or 'False'. Disregard any potential nonce words for your answer. \\nStatement: \" # consider adding instructions regarding the nonce words\n",
    "#system_prompt_comps = \"Is the conclusion of the following statement true or false? Simply answer with 'True' or 'False'. \\nStatement: \" # consider adding instructions regarding the nonce words\n",
    "system_prompt = \"Given the following premises, please determine for which one the conclusion can be drawn from:\\n\\n\"\n",
    "\n",
    "# s2a step 1 prompt is loosely based on W&S (2023), Fig. 15\n",
    "fig_15 = '''Given the following text by a user, extract the part that is related and useful, so that\n",
    "using that text alone would be good context for providing an accurate and correct answer\n",
    "to the question portion of the text. Please include the actual question or query that the\n",
    "user is asking. Separate this into two categories labeled with ”Context text related to the\n",
    "question (includes all content except unrelated sentences):” and ”Detailed question:”. Do\n",
    "not use list.'''\n",
    "\n",
    "# step 1: separate relevant from distracting information \n",
    "if dataset == 'noise':\n",
    "    label = \"Context text related to the incomplete sentence (includes all content except unrelated sentences):\"\n",
    "    s2a_step1 = f'''Given the following incomplete sentence, extract the part that is related and useful, so that using that text alone would a be good context for providing an accurate completion to the incomplete sentence. Label your response with \"{label}\".\n",
    "    \n",
    "    Text by user: '''\n",
    "\n",
    "if dataset == 'comps':\n",
    "    label = \"Context relevant to conlcusion (includes all content except irrelevant sentences):\"\n",
    "    s2a_step1 = f'''Given the following text by a user, extract the part that is related and useful, so that using that text alone would be sufficient context for stating whether the conclusion is true or false. Label your response with \"{label}\". \\nText by user: '''\n",
    "\n",
    "if dataset in ['qa', 'qa_ic']:\n",
    "    label = \"Question (does not include irrelevant context):\"\n",
    "    s2a_step1 = f'''Given the following text by a user, remove the the portion that is irrelevant to answer the question. Label your response with \"{label}\".\n",
    "    \n",
    "    Text by User: '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given the following text by a user, extract the part that is related and useful, so that using that text alone would be sufficient context for stating whether the conclusion is true or false. Label your response with \"Context relevant to conlcusion (includes all content except irrelevant sentences):\". \\nText by user: '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards:  33%|███▎      | 5/15 [01:11<02:22, 14.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m s2a \u001b[38;5;241m=\u001b[39m reload(s2a)\n\u001b[1;32m      2\u001b[0m max_memory_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m36GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m6\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m36GB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m7\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m30GB\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43ms2a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLLAMA2_70B_CHAT_HF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_pinning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/js/lmcm_project/s2a.py:202\u001b[0m, in \u001b[0;36mload_model_hf\u001b[0;34m(model_id, memory_pinning, in_4bit, not_quant)\u001b[0m\n\u001b[1;32m    197\u001b[0m     bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m    198\u001b[0m         load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    201\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m], padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m--> 202\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_pinning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHF_TOKEN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/transformers/modeling_utils.py:807\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    805\u001b[0m         set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;66;03m# TODO: consider removing used param_parts from state_dict before return\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:213\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    210\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    212\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 213\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:191\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/local/js/anaconda3/envs/main/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:168\u001b[0m, in \u001b[0;36mParams4bit.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[0;32m--> 168\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mquantize_4bit(w, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, compress_statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompress_statistics, quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_type)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "max_memory_mapping = {0: \"0GB\", 1: \"0GB\", 2: \"0GB\", 3:\"0GB\", 4:\"0GB\", 5:\"36GB\", 6:\"36GB\", 7:\"30GB\"}\n",
    "model, tokenizer = s2a.load_model_hf(LLAMA2_70B_CHAT_HF, memory_pinning=max_memory_mapping, in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "s2a = reload(s2a)\n",
    "sents, targets = (comps_base['sentence_acc'], comps_base['sentence_unacc']), (comps_base['acceptable_concept'], comps_base['unacceptable_concept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁[', 'INST', ']', '▁<<', 'SY', 'S', '>>', '<0x0A>', 'G', 'iven', '▁the', '▁following', '▁text', '▁by', '▁a', '▁user', ',', '▁extract', '▁the', '▁part', '▁that', '▁is', '▁related', '▁and', '▁useful', ',', '▁so', '▁that', '▁using', '▁that', '▁text', '▁alone', '▁would', '▁be', '▁sufficient', '▁context', '▁for', '▁stating', '▁whether', '▁the', '▁conclusion', '▁is', '▁true', '▁or', '▁false', '.', '▁Label', '▁your', '▁response', '▁with', '▁\"', 'Context', '▁relevant', '▁to', '▁con', 'l', 'c', 'usion', '▁(', 'includes', '▁all', '▁content', '▁except', '▁irrelevant', '▁sentences', '):', '\".', '▁', '<0x0A>', 'Text', '▁by', '▁user', ':', '▁', '<0x0A>', '<', '</', 'SY', 'S', '>>', '<0x0A>', '<0x0A>', 'A', '▁w', 'ug', '▁is', '▁a', '▁muss', 'el', '.', '▁Therefore', ',', '▁a', '▁w', 'ug', '▁attach', 'es', '▁to', '▁rocks', '.', '▁[', '/', 'INST', ']']\n"
     ]
    }
   ],
   "source": [
    "# step 1 s2a\n",
    "data = s2a.preprocess(sents, tokenizer, model.name_or_path, s2a_step1)\n",
    "print(tokenizer.convert_ids_to_tokens(data['input_ids'][0], skip_special_tokens=True))\n",
    "#file_spec = f\"{dataset}_{data_subset}_s2a_step1_{model_name}\"\n",
    "\n",
    "N = 2\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(data, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "#preds = s2a.generate_hf(model, tokenizer, dataloader, sents, targets, N=N, out=False, file_spec=None)\n",
    "#sents = s2a.post_proc_s2a(preds, dataset, label)\n",
    "#file_spec = f\"{dataset}_{data_subset}_s2a_step2_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁[', 'INST', ']', '▁<<', 'SY', 'S', '>>', '<0x0A>', 'Is', '▁the', '▁conclusion', '▁of', '▁the', '▁following', '▁statement', '▁true', '▁or', '▁false', '?', '▁Just', '▁answer', '▁with', \"▁'\", 'True', \"'\", '▁or', \"▁'\", 'False', \"'.\", '▁Dis', 'reg', 'ard', '▁any', '▁potential', '▁non', 'ce', '▁words', '▁for', '▁your', '▁answer', '.', '▁', '<0x0A>', 'Statement', ':', '▁', '<0x0A>', '<', '</', 'SY', 'S', '>>', '<0x0A>', '<0x0A>', 'A', '▁w', 'ug', '▁is', '▁a', '▁muss', 'el', '.', '▁Therefore', ',', '▁a', '▁w', 'ug', '▁attach', 'es', '▁to', '▁rocks', '.', '▁[', '/', 'INST', ']']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 2/1737 [01:10<17:00:44, 35.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' False',\n",
       " ' False',\n",
       " ' False',\n",
       " ' False',\n",
       " ' True',\n",
       " ' True',\n",
       " ' True',\n",
       " ' True',\n",
       " ' False',\n",
       " ' False',\n",
       " ' False',\n",
       " ' False.\\n\\nThe statement is trying to trick the reader into assuming that \"blicket',\n",
       " ' True',\n",
       " ' True',\n",
       " ' True',\n",
       " ' True',\n",
       " ' False.\\n\\nThe statement \"A fep is a crocodile\" is not true',\n",
       " ' True',\n",
       " ' False',\n",
       " ' True',\n",
       " ' True',\n",
       " ' False',\n",
       " ' True',\n",
       " ' False']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prepare data for step 2 s2a\n",
    "s2a = reload(s2a)\n",
    "data = s2a.preprocess(sents, tokenizer, model.name_or_path, system_prompt_comps)\n",
    "\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(data, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "print(tokenizer.convert_ids_to_tokens(data['input_ids'][0], skip_special_tokens=True))\n",
    "preds = s2a.generate_hf(model, tokenizer, dataloader, sents, targets, N=3) #, out=True, file_spec='multiSem_baseline_oracle_Llama-2-70b-chat-hf')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "Explanation:\n",
      "The sentence is false because the conclusion \"A wug attaches to rocks\" does not logically follow from the premises. While it is true that some clams and mussels can attach to rocks, it is not a characteristic that is unique to these two types of shellfish. Many other types of shellfish, such as barnacles and oysters, can also attach to rocks. Therefore, the deduction is not valid, and the sentence is false.\n",
      "False\n",
      "False\n",
      "False\n",
      "False. The sentence is a play on words and does not convey a valid statement.\n",
      "False\n",
      "False\n",
      "True\n",
      "contained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import s2a\n",
    "\n",
    "s2a = reload(s2a)\n",
    "path = '/local/js/lmcm_project/eval/comps_before_baseline_oracle_llama2-70.csv'\n",
    "eval_df = pd.read_csv(path)\n",
    "preds, targets = eval_df['prediction'], eval_df['target']\n",
    "#print([\"Final answer (true or false):\" in pred for pred in preds])\n",
    "s2a.accuracy(preds, targets, comps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is-in Accuracy:\t\t0.92 \n",
      "Exact Match Accuracy:\t0.92 \n",
      "First Word Accuracy:\t0.92\n"
     ]
    }
   ],
   "source": [
    "s2a = reload(s2a)\n",
    "file_spec = \"singleSem_baseline_oracle_llama2-70\"\n",
    "path = f\"/local/js/lmcm_project/eval/{file_spec}.csv\"\n",
    "eval_df = pd.read_csv(path)\n",
    "preds, targets = eval_df['prediction'], eval_df['target']\n",
    "preds = s2a.post_proc(preds)\n",
    "acc_isin = s2a.accuracy(preds, targets, isin=True)\n",
    "acc_em = s2a.accuracy(preds, targets, exact_match=True)\n",
    "acc_fw = s2a.accuracy(preds, targets, first_word=True)\n",
    "\n",
    "print(f\"Is-in Accuracy:\\t\\t{acc_isin} \\nExact Match Accuracy:\\t{acc_em} \\nFirst Word Accuracy:\\t{acc_fw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_bit_memory = 4551360512\n",
    "with_acc_4bit = 7241732096\n",
    "mistral_wo_quant = 30040678400\n",
    "llama_memory = 26462937088\n",
    "mistral_wo_quant/llama_memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
